# Конфигурация для парсера датасетов HuggingFace
# Предназначен для подготовки данных для обучения русскоязычных трансформеров

# Список датасетов для обработки
datasets:
  # Пример: Датасет с русскими текстами общего назначения

  - name: "cointegrated/taiga_stripped_rest"           # Название датасета на HuggingFace
    split: "Arzamas, Fontanka, Interfax, KP, Lenta, Magazines, NPlus1, Subtitles, social"                              # Какой split использовать (train/validation/test)
    subset: null                                # Подмножество датасета (null если не нужно)
    text_column: "text"                         # Название колонки с текстом
    tag: "<GENERAL>"


# Можно вставить несколько датасетов, из них будет в итоге сформирован единный бинарник готовый для обучения.
#  - name: "cointegrated/taiga_stripped_rest "
#    split: "train"
#    subset: "ru"
#    text_column: "text"
#    tag: "<Some_Tag>"

# Настройки фильтрации и обработки текста
filtering:
  # Минимальная длина текста в символах
  # Рассчитано для 256 токенов с максимум 30% padding:
  # 256 * 0.7 = 179 полезных токенов
  # 179 / 2 токена на слово = ~90 слов
  # 90 слов * 6 символов = 540 символов минимум
  min_length: 540

  # Максимальная длина текста в символах
  # Тексты длиннее будут разделены на части
  max_length: 2000

  # Размер части при разделении длинных текстов
  # Должен быть меньше max_length для корректной работы
  chunk_size: 1500

  # Размер перекрытия между частями при разделении (в символах)
  # Помогает сохранить контекст между частями
  # Примерно 30-40 слов для плавного перехода
  chunk_overlap: 0

  # Удалять записи содержащие Unicode эмодзи
  # True = удалять записи с эмодзи (рекомендуется для избежания мисалайнмента)
  # False = оставлять эмодзи в тексте
  remove_emoji: true

  # Проверять что текст содержит только допустимые символы
  # True = удалять записи с символами кроме кириллицы, латиницы, цифр, пунктуации
  # False = разрешить любые символы
  # Рекомендуется True для маленьких моделей (120-200M параметров)
  allowed_chars_only: true

# Настройки выходного файла
output:
  # Путь к файлу куда сохранять обработанные данные
  # Этот файл будет использоваться токенизатором
  file_path: "data/parsed_dataset.txt"